name: Initialize LSTM Model
description: Initializes the LSTM model with optimized configurations for Reddit adoption chasm classification and standard LSTM configurations.
inputs:
  - {name: config, type: String}
outputs:
  - {name: model, type: Model}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v30
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import sys
        
        from nesy_factory.RNNs import LSTM

        def get_adoption_model_config(config):
            
            # Optimized configuration for adoption chasm using standard LSTM
            model_config = {
                'input_dim': len(config['feature_columns']),
                'hidden_dim': config.get('hidden_dim', 128),  # Larger for complex adoption patterns
                'output_dim': 1,  # Single adoption score output
                'num_layers': config.get('num_layers', 2),  # Deeper for sequential patterns
                'dropout': config.get('dropout', 0.3),  # Regularization for adoption data
                'optimizer': 'adam',
                'learning_rate': config.get('learning_rate', 0.001),
                'epochs': config.get('epochs', 100),  # More epochs for adoption learning
                'loss_function': config.get('loss_function', 'MSELoss'),  # Regression for adoption score
                'use_case': config.get('use_case', 'Reddit_Adoption_Chasm'),
                'task_type': config.get('task_type', 'adoption_stage_classification')
            }
            
            return model_config

        def get_standard_model_config(config):
            
            model_config = {
                'input_dim': len(config['feature_columns']),
                'hidden_dim': config.get('hidden_dim', 64),
                'output_dim': config.get('output_dim', 1),
                'num_layers': config.get('num_layers', 1),
                'dropout': config.get('dropout', 0.2),
                'optimizer': config.get('optimizer', 'adam'),
                'learning_rate': config.get('learning_rate', 0.01),
                'epochs': config.get('epochs', 10),
                'loss_function': config.get('loss_function', 'MSELoss'),
                'use_case': config.get('use_case', 'Unknown'),
                'task_type': config.get('task_type', 'regression')
            }
            
            return model_config

        def create_model(config):
            
            if config.get('use_case') == 'Reddit_Adoption_Chasm' or config.get('task_type') == 'adoption_stage_classification':
                # Use optimized standard LSTM for adoption chasm
                model_config = get_adoption_model_config(config)
                model_obj = LSTM(model_config)
                print(f"Created Optimized LSTM for Reddit Adoption Chasm Classification")
                print(f"  Input Dimension: {model_config['input_dim']}")
                print(f"  Hidden Dimension: {model_config['hidden_dim']}")
                print(f"  Output Dimension: {model_config['output_dim']}")
                print(f"  Number of Layers: {model_config['num_layers']}")
                print(f"  Task Type: {model_config.get('task_type', 'adoption_stage_classification')}")
                
            else:
                # Use standard LSTM for other use cases
                model_config = get_standard_model_config(config)
                model_obj = LSTM(model_config)
                print(f"Created Standard LSTM for {config.get('use_case', 'Unknown')} use case")
                print(f"  Input Dimension: {model_config['input_dim']}")
                print(f"  Hidden Dimension: {model_config['hidden_dim']}")
                print(f"  Output Dimension: {model_config['output_dim']}")
                print(f"  Number of Layers: {model_config['num_layers']}")
            
            return model_obj

        parser = argparse.ArgumentParser()
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--model', type=str, required=True)
        args = parser.parse_args()

        config = json.loads(args.config)
        
        print(f"Initializing LSTM Model for use case: {config.get('use_case', 'Unknown')}")
        print(f"Task type: {config.get('task_type', 'regression')}")
        print(f"Number of features: {len(config['feature_columns'])}")

        # Create the appropriate model
        model_obj = create_model(config)

        # Save the model
        os.makedirs(os.path.dirname(args.model), exist_ok=True)
        with open(args.model, "wb") as f:
            pickle.dump(model_obj, f)

        print(f"Successfully saved LSTM model to {args.model}")
        
        # Print model summary
        model_config = model_obj.config if hasattr(model_obj, 'config') else config
            
        print(f"\\nModel Configuration Summary:")
        key_configs = ['use_case', 'task_type', 'hidden_dim', 'num_layers', 'dropout', 'learning_rate', 'optimizer', 'loss_function']
        for key in key_configs:
            if key in model_config:
                print(f"  {key}: {model_config[key]}")
    args:
      - --config
      - {inputValue: config}
      - --model
      - {outputPath: model}
