name: DQN RLAF Loop for LSTM
description: Triggers the DQN RLAF pipeline in a loop to optimize LSTM model hyperparameters, controlled by a pierce_or_not flag.
inputs:
  - {name: trained_model, type: Model}
  - {name: init_metrics, type: Metrics}
  - {name: data_path, type: Dataset}
  - {name: config, type: String}
  - {name: domain, type: String}
  - {name: schema_id, type: String}
  - {name: model_id, type: String}
  - {name: dqn_pipeline_id, type: String}
  - {name: pipeline_domain, type: String}
  - {name: dqn_experiment_id, type: String}
  - {name: access_token, type: string}
  - {name: tasks, type: Dataset}
outputs:
  - {name: rlaf_output, type: Dataset}
  - {name: retrained_model, type: Model}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v25
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import os
        import json
        import argparse
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import time
        import pickle
        from typing import List, Dict, Any
        import numpy as np
        from nesy_factory.RNNs import LSTM
        from torch.utils.data import TensorDataset, DataLoader

        class DataWrapper:
          def __init__(self, data_dict):
              self.__dict__.update(data_dict)

        class StandardScaler():
            def __init__(self, mean, std):
                self.mean = mean
                self.std = std
            def transform(self, data):
                return (data - self.mean) / self.std
            def inverse_transform(self, data):
                return (data * self.std) + self.mean
        
        class TemporalDataSplitter:
          
          def __init__(self, data, config, strategy='temporal_split'):
              self.data = data
              self.config = config
              self.strategy = strategy
              
          def create_continual_tasks(self, num_tasks: int = 3) -> List[Dict]:
              return self._temporal_split(num_tasks)
          
          def _temporal_split(self, num_tasks: int) -> List[Dict]:
              tasks = []
              
              train_size = len(self.data['X_train'])
              test_size = len(self.data['X_test'])
              
              train_splits = np.array_split(range(train_size), num_tasks)
              test_splits = np.array_split(range(test_size), num_tasks)
              
              for i in range(num_tasks):
                  task_data = {
                      'task_id': i,
                      'X_train': self.data['X_train'][train_splits[i]],
                      'y_train': self.data['y_train'][train_splits[i]],
                      'X_test': self.data['X_test'][test_splits[i]],
                      'y_test': self.data['y_test'][test_splits[i]],
                      'description': f'LSTM Temporal Period {i+1}/{num_tasks}'
                  }
                  
                  train_dataset = TensorDataset(torch.tensor(task_data['X_train'], dtype=torch.float32), torch.tensor(task_data['y_train'], dtype=torch.float32).unsqueeze(1))
                  test_dataset = TensorDataset(torch.tensor(task_data['X_test'], dtype=torch.float32), torch.tensor(task_data['y_test'], dtype=torch.float32).unsqueeze(1))

                  task_data['train_loader'] = DataLoader(train_dataset, batch_size=self.config['batch_size'], shuffle=True)
                  task_data['test_loader'] = DataLoader(test_dataset, batch_size=self.config['batch_size'], shuffle=False)
                  
                  tasks.append(task_data)
              
              return tasks

        class ContinualLSTMTrainer:
            
            def __init__(self, config: Dict[str, Any]):
                self.config = config
                self.results = {}
                
            def train_continual_lstm(
                self, 
                tasks: List[Dict], 
                model, 
                strategies: List[str] = ['naive', 'replay', 'regularized']
            ) -> Dict[str, Any]:
                results = {}
                
                for strategy_name in strategies:
                    print(f'Training LSTM with {strategy_name.upper()} strategy')
                    strategy_results = self._train_single_strategy(tasks, strategy_name, model)
                    results[strategy_name] = strategy_results
                    
                return results
            
            def _train_single_strategy(self, tasks: List[Dict], strategy_name: str, model) -> Dict[str, Any]:
                
                task_metrics = []
                all_task_performance = []
                previous_task_data = []
                
                print(f"Learning {len(tasks)} sequential LSTM tasks")
                
                for task_idx, task_data in enumerate(tasks):
                    print(f"Learning Task {task_idx + 1}: {task_data['description']}")
                    
                    if strategy_name == 'naive':
                        training_loader = task_data['train_loader']
                    elif strategy_name == 'replay':
                        if previous_task_data:
                            training_loader = self._create_replay_loader(task_data, previous_task_data)
                        else:
                            training_loader = task_data['train_loader']
                    elif strategy_name == 'regularized':
                        training_loader = task_data['train_loader']
                        if task_idx > 0:
                            self._store_important_params(model)
                    else:
                        training_loader = task_data['train_loader']
                    
                    print(f"  Training on Task {task_idx + 1}")
                    for epoch in range(self.config.get('epochs', 10)):
                        model.train()
                        total_loss = 0
                        for inputs, labels in training_loader:
                            if strategy_name == 'regularized' and task_idx > 0:
                                loss = self._train_with_regularization(model, (inputs, labels))
                            else:
                                loss = model.train_step((inputs, labels))
                            total_loss += loss
                        avg_loss = total_loss / len(training_loader)
                        if epoch % 5 == 0:
                            print(f"    Epoch {epoch:03d} | Loss: {avg_loss:.4f}")
                    
                    if strategy_name == 'replay':
                        previous_task_data.append(task_data)
                        if len(previous_task_data) > 3:
                            previous_task_data = previous_task_data[-3:]
                    
                    current_task_metrics = model.eval_step(task_data['test_loader'])
                    task_metrics.append(current_task_metrics)
                    
                    print(f"     Task {task_idx + 1} Test Loss: {current_task_metrics['loss']:.4f}")
                    
                    task_performance = []
                    for eval_task_idx in range(task_idx + 1):
                        eval_metrics = model.eval_step(tasks[eval_task_idx]['test_loader'])
                        task_performance.append({
                            'task_id': eval_task_idx,
                            'loss': eval_metrics['loss'],
                            'description': tasks[eval_task_idx]['description']
                        })
                    
                    all_task_performance.append(task_performance)
                    
                    if task_idx > 0:
                        print(f"Performance on previous tasks:")
                        for prev_task in task_performance[:-1]:
                            print(f"    Task {prev_task['task_id'] + 1}: {prev_task['loss']:.4f}")
                
                cl_metrics = self._calculate_continual_metrics(all_task_performance)
                
                final_eval_metrics = [model.eval_step(tasks[i]['test_loader']) for i in range(len(tasks))]
                
                avg_metrics = {}
                if final_eval_metrics:
                    for key in final_eval_metrics[0]:
                        avg_metrics[key] = np.mean([m[key] for m in final_eval_metrics])

                results = {
                    'strategy': strategy_name,
                    'task_metrics': task_metrics,
                    'all_task_performance': all_task_performance,
                    'continual_metrics': cl_metrics,
                    'final_model': model,
                    'average_eval_metrics': avg_metrics
                }
                
                return results
            
            def _create_replay_loader(self, current_task: Dict, previous_tasks: List[Dict]) -> DataLoader:
                replay_ratio = 0.3
                
                current_X = current_task['X_train']
                current_y = current_task['y_train']
                current_size = len(current_X)
                replay_size = int(current_size * replay_ratio / (1 - replay_ratio))
                
                replay_x = []
                replay_y = []
                
                for prev_task in previous_tasks:
                    if len(prev_task['X_train']) > 0:
                        sample_size = min(replay_size // len(previous_tasks), len(prev_task['X_train']))
                        if sample_size > 0:
                            indices = np.random.choice(len(prev_task['X_train']), sample_size, replace=False)
                            replay_x.append(prev_task['X_train'][indices])
                            replay_y.append(prev_task['y_train'][indices])
                
                if replay_x:
                    combined_x = np.concatenate([current_X] + replay_x)
                    combined_y = np.concatenate([current_y] + replay_y)
                else:
                    combined_x = current_X
                    combined_y = current_y
                
                replay_dataset = TensorDataset(torch.tensor(combined_x, dtype=torch.float32), torch.tensor(combined_y, dtype=torch.float32).unsqueeze(1))
                return DataLoader(replay_dataset, batch_size=self.config['batch_size'], shuffle=True)

            def _store_important_params(self, model):
                if not hasattr(self, 'important_params'):
                    self.important_params = {}
                
                for name, param in model.named_parameters():
                    if param.requires_grad:
                        self.important_params[name] = param.clone().detach()
            
            def _train_with_regularization(self, model, data) -> float:
                base_loss = model.train_step(data)
                
                reg_loss = 0.0
                reg_lambda = 0.1
                
                if hasattr(self, 'important_params'):
                    for name, param in model.named_parameters():
                        if name in self.important_params and param.requires_grad:
                            reg_loss += torch.sum((param - self.important_params[name]).pow(2))
                
                total_loss = base_loss + reg_lambda * reg_loss
                
                model.optimizer.zero_grad()
                total_loss.backward()
                model.optimizer.step()

                return total_loss.item()
            
            def _calculate_continual_metrics(self, all_task_performance: List[List[Dict]]) -> Dict[str, float]:
                
                final_performance = all_task_performance[-1]
                average_loss = np.mean([task['loss'] for task in final_performance])
                
                backward_transfers = []
                if len(all_task_performance) > 1:
                    for task_idx in range(len(all_task_performance) - 1):
                        initial_loss = all_task_performance[task_idx][task_idx]['loss']
                        final_loss = all_task_performance[-1][task_idx]['loss']
                        backward_transfer = initial_loss - final_loss
                        backward_transfers.append(backward_transfer)
                
                avg_backward_transfer = np.mean(backward_transfers) if backward_transfers else 0.0
                
                forgetting_scores = []
                for task_idx in range(len(all_task_performance) - 1):
                    min_loss = all_task_performance[task_idx][task_idx]['loss']
                    final_loss = all_task_performance[-1][task_idx]['loss']
                    forgetting = final_loss - min_loss
                    forgetting_scores.append(max(0, forgetting))
                
                avg_forgetting = np.mean(forgetting_scores) if forgetting_scores else 0.0
                
                return {
                    'average_loss': average_loss,
                    'backward_transfer': avg_backward_transfer,
                    'forgetting': avg_forgetting,
                    'num_tasks': len(all_task_performance)
                }
        
        #  API/DB Helper Functions
        def get_retry_session():
            retry_strategy = Retry(
                total=5,
                status_forcelist=[500, 502, 503, 504],
                backoff_factor=1
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session = requests.Session()
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            return session

        def trigger_pipeline(config, pipeline_domain, model_id, dqn_params=None):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={config['pipeline_id']}"
            pipeline_params = {"param_json": json.dumps(dqn_params), "model_id": model_id} if dqn_params else {"model_id": model_id}
            payload = json.dumps({
                "pipelineType": "ML", "containerResources": {}, "experimentId": config['experiment_id'],
                "enableCaching": True, "parameters": pipeline_params, "version": 1
            })
            print(f"{payload}")
            headers = {
                'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}",
                'Content-Type': 'application/json'
            }
            print(f"Trigger pipeline request URL: {url}, Headers: {headers}, Payload: {payload}")
            response = http.post(url, headers=headers, data=payload, timeout=30)
            response.raise_for_status()
            print(f"Triggered pipeline. Status Code: {response.status_code}, Response: {response.json()}")
            return response.json()['runId']

        def get_pipeline_status(config, pipeline_domain):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/{config['pipeline_id']}/status/ml/{config['run_id']}"
            headers = {'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}"}
            print(f"Get pipeline status request URL: {url}, Headers: {headers}")
            response = http.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            pipeline_status = response.json()
            print(f"Get pipeline status. Status Code: {response.status_code}, Full response: {pipeline_status}")
            latest_state = pipeline_status['run_details']['state_history'][-1]
            return latest_state['state']

        def get_instance(access_token, domain, schema_id, model_id):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {"dbType": "TIDB", "ownedOnly": True, "filter": {"model_id": model_id}}
            print(f"Get instance request URL: {url}, Headers: {headers}, Payload: {payload}")
            response = http.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            response_data = response.json()
            print(f"Get instance. Status Code: {response.status_code}, Full response: {response_data}")
            
            # Check if content exists and has at least one instance
            if 'content' not in response_data or not response_data['content']:
                raise ValueError(f"No instances found for model_id: {model_id}. Response: {response_data}")
            
            return response_data['content'][0]

        def create_instance(access_token, domain, schema_id, model_id):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {
                "dbType": "TIDB",
                "instances": [{
                    "model_id": model_id,
                    "pierce2rlaf": [],
                    "rlaf2pierce": [],
                    "rlaf_actions": {"actions": []}
                }]
            }
            print(f"Create instance request - URL: {url}, Headers: {headers}")
            response = http.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            print(f"Create instance. Status Code: {response.status_code}")
            return response.json()

        def update_instance_field(access_token, domain, schema_id, model_id, field, value):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {
                "dbType": "TIDB",
                "conditionalFilter": {"conditions": [{"field": "model_id", "operator": "EQUAL", "value": model_id}]},
                "partialUpdateRequests": [{"patch": [{"operation": "REPLACE", "path": f"{field}", "value": value}]}]
            }
            print(f"Update instance field request - URL: {url}, Headers: {headers}, Payload: {json.dumps(payload)}")
            headers_str = " ".join([f"-H '{k}: {v}'" for k, v in headers.items()])
            print(f"curl -X PATCH '{url}' {headers_str} -d '{json.dumps(payload)}'")
            response = http.patch(url, headers=headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()
            print(f"Update instance field. Status Code: {response.status_code}, Full response: {response.json()}")
            print(f"Successfully updated field '{field}' for model_id '{model_id}'")

        #  Core Logic 
        def trigger_and_wait_for_dqn_pipeline(config, pipeline_domain, model_id, dqn_params):
            run_id = trigger_pipeline(config, pipeline_domain, model_id, dqn_params)
            config["run_id"] = run_id
            while True:
                status = get_pipeline_status(config, pipeline_domain)
                print(f"Current DQN pipeline status: {status}")
                if status == 'SUCCEEDED':
                    print("DQN Pipeline execution completed.")
                    break
                elif status in ['FAILED', 'ERROR']:
                    raise RuntimeError(f"DQN Pipeline failed with status {status}")
                time.sleep(60)

        def model_retraining(action, model_path, data_path, config, tasks_path, output_model_path, previous_metrics, dqn_params):
            with open(data_path, "rb") as f: data = pickle.load(f)
            with open(tasks_path, "rb") as f: tasks = pickle.load(f)
            
            config.update(action)
            
            model_config = {
                'input_dim': len(config['feature_columns']),
                'hidden_dim': config['hidden_dim'],
                'output_dim': 1,
                'num_layers': config['num_layers'],
                'dropout': config['dropout'],
                'optimizer': 'adam',
                'learning_rate': config['learning_rate'],
                'epochs': config['epochs'],
                'loss_function': config['loss_function']
            }

            model = LSTM(model_config)
            model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
            model._init_optimizer_and_criterion()

            print("Starting LSTM continual learning experiments")
            trainer = ContinualLSTMTrainer(config)
            continual_strategies = ['naive']
            
            results = trainer.train_continual_lstm(tasks=tasks, strategies=continual_strategies, model=model)
            print(f"{results}")
            
            average_eval_metrics = results['naive']['average_eval_metrics']
            
            improvement_score = 0
            for param in dqn_params:
                key = param['key']
                sign = 1 if param['sign'] == '+' else -1
                if key in average_eval_metrics and key in previous_metrics:
                    improvement = (average_eval_metrics[key] - previous_metrics[key]) * sign
                    improvement_score += improvement

            if improvement_score > 0:
                print(f"LSTM metrics improved (score: {improvement_score:.4f}). Saving model.")
                os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
                final_model = results['naive']['final_model']
                torch.save(final_model.state_dict(), output_model_path)
                print(f"Saved retrained LSTM model to {output_model_path}")
            else:
                print(f"No improvement in LSTM metrics (score: {improvement_score:.4f}). Model not saved.")
                os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
                with open(output_model_path, 'w') as f:
                    f.write("LSTM model not saved due to lack of improvement.")

            return {"metrics": average_eval_metrics, "model_path": output_model_path}


        #  Main Execution
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--init_metrics', type=str, required=True)
            parser.add_argument('--rlaf_output', type=str, required=True)
            parser.add_argument('--data_path', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--domain', type=str, required=True)
            parser.add_argument('--schema_id', type=str, required=True)
            parser.add_argument('--model_id', type=str, required=True)
            parser.add_argument('--dqn_pipeline_id', type=str, required=True)
            parser.add_argument('--dqn_experiment_id', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            parser.add_argument('--tasks', type=str, required=True)
            parser.add_argument('--pipeline_domain', type=str, required=True)
            parser.add_argument('--retrained_model', type=str, required=True)
            args = parser.parse_args()

            with open(args.access_token, 'r') as f:
                access_token = f.read().strip()
            with open(args.init_metrics, 'r') as f:
                current_metrics = json.load(f)
 
            action_id_for_next_pierce = -1
 
            for i in range(2):
                print(f"LSTM RLAF Loop Iteration {i+1}")
                
                cleaned_metrics = {}
                dqn_params = []
                for key, value in current_metrics.items():
                    try:
                        cleaned_metrics[key] = float(value)
                        if "loss" in key.lower() or "accuracy" in key.lower() or "mse" in key.lower() or "mae" in key.lower():
                            sign = "+" if "accuracy" in key.lower() or "f1" in key.lower() else "-"
                            dqn_params.append({"key": key, "sign": sign, "mul": 1.0})
                    except (ValueError, TypeError):
                        print(f"Warning: Could not convert metric '{key}' with value '{value}' to float. Skipping.")
                
                print(f"Dynamically generated param_json for DQN: {json.dumps(dqn_params)}")

                # Get instance with error handling
                try:
                    instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                    print(f"Retrieved instance: {instance}")
                except ValueError as e:
                    print(f"Error retrieving instance: {e}")
                    print("Creating initial instance structure...")
                    
                    # Create a default instance structure
                    instance = {
                        'pierce2rlaf': [],
                        'rlaf2pierce': [],
                        'rlaf_actions': {'actions': []}
                    }
                    
                    # Try to create the instance if it doesn't exist
                    try:
                        create_instance(access_token, args.domain, args.schema_id, args.model_id)
                        print("Created new instance for model_id")
                    except Exception as create_error:
                        print(f"Warning: Could not create instance: {create_error}")
                        print("Using default instance structure for this run")

                if instance.get('pierce2rlaf'):
                    latest_pierce2rlaf = instance['pierce2rlaf'][-1]
                    previous_state = latest_pierce2rlaf['current_state']
                    episode = latest_pierce2rlaf['episode']
                else:
                    previous_state = {key: 0.0 for key in cleaned_metrics.keys()}
                    episode = 0
 
                new_pierce2rlaf_entry = {
                    "action_id": action_id_for_next_pierce, "previous_state": previous_state,
                    "current_state": cleaned_metrics, "episode": episode, "timestamp": int(time.time())
                }
                pierce2rlaf_history = instance.get("pierce2rlaf", [])
                pierce2rlaf_history.append(new_pierce2rlaf_entry)
                
                # Only update if we have a real instance
                if 'model_id' in instance:
                    update_instance_field(access_token, args.domain, args.schema_id, args.model_id, "pierce2rlaf", pierce2rlaf_history)

                dqn_config = {
                    "pipeline_id": args.dqn_pipeline_id, "experiment_id": args.dqn_experiment_id, "access_token": access_token
                }
                print(f"{dqn_config}")
                trigger_and_wait_for_dqn_pipeline(dqn_config, args.pipeline_domain, args.model_id, dqn_params)

                # Get updated instance
                try:
                    updated_instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                except ValueError as e:
                    print(f"Error retrieving updated instance: {e}")
                    print("Exiting LSTM RLAF loop.")
                    break
                
                if 'rlaf2pierce' not in updated_instance or len(updated_instance['rlaf2pierce']) == 0:
                    print("No rlaf2pierce data found. Exiting LSTM RLAF loop.")
                    break
                
                latest_rlaf2pierce = updated_instance['rlaf2pierce'][-1]
                
                if not latest_rlaf2pierce.get("pierce_or_not", True):
                    print("pierce_or_not is false. Exiting LSTM RLAF loop.")
                    break
                
                print(f"{latest_rlaf2pierce}")
                rlaf_actions = updated_instance.get('rlaf_actions', {}).get('actions', [])
                action_id_for_next_pierce = latest_rlaf2pierce['action_id']
                action_details = next((a for a in rlaf_actions if a["id"] == action_id_for_next_pierce), None)
                if not action_details:
                    print(f"Warning: Action with ID {action_id_for_next_pierce} not found in rlaf_actions")
                    break
 
                print(f"DQN pipeline recommended action for LSTM: {action_details}. Retraining model.")
                retraining_results = model_retraining(
                    action_details['params'], args.trained_model, args.data_path, json.loads(args.config), args.tasks,
                    args.retrained_model, previous_state, dqn_params
                )

                # Ensure output directory exists and file is created
                os.makedirs(os.path.dirname(args.retrained_model), exist_ok=True)
                if not os.path.exists(args.retrained_model):
                    # Create a placeholder file if model wasn't saved due to no improvement
                    with open(args.retrained_model, 'w') as f:
                        f.write("no_improvement")

                current_metrics = retraining_results["metrics"]

            # Ensure output directory exists
            os.makedirs(os.path.dirname(args.rlaf_output), exist_ok=True)
            with open(args.rlaf_output, 'w') as f:
                json.dump({"final_metrics": current_metrics}, f, indent=4)
            print(f"LSTM RLAF loop finished. Final parameters written to {args.rlaf_output}")

            # Also ensure retrained_model output exists if not created
            if not os.path.exists(args.retrained_model):
                with open(args.retrained_model, 'w') as f:
                    f.write("no_improvement_final")

        if __name__ == '__main__':
            main()
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --init_metrics
      - {inputPath: init_metrics}
      - --rlaf_output
      - {outputPath: rlaf_output}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --domain
      - {inputValue: domain}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --access_token
      - {inputPath: access_token}
      - --tasks
      - {inputPath: tasks}
      - --pipeline_domain
      - {inputValue: pipeline_domain}
      - --retrained_model
      - {outputPath: retrained_model}
