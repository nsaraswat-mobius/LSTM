name: Fetch Inference Input
description: Fetches inference input data from a specified API endpoint using schema_id and execution_id.
inputs:
  - {name: schema_id, type: String, description: 'The schema ID to fetch data from.'}
  - {name: execution_id, type: String, description: 'The execution ID to filter the data.'}
  - {name: token, type: string, description: 'Bearer token for authentication.'}
  - {name: url_domain, type: String, description: 'The domain for the API endpoint.'}
outputs:
  - {name: out_pickle, type: Dataset}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet requests || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet requests --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import requests
        import pickle
        import ast

        def fetch_data_from_api(schema_id, execution_id, token, url_domain):
            url = f'https://{url_domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list'
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {token}'
            }
            data = {
              "dbType": "TIDB",
              "ownedOnly": False,
              "filter": {
                  "execution_id": execution_id
              }
            }
            response = requests.post(url, headers=headers, data=json.dumps(data))
            response.raise_for_status()
            return response.json()

        parser = argparse.ArgumentParser()
        parser.add_argument('--schema_id', type=str, required=True)
        parser.add_argument('--execution_id', type=str, required=True)
        parser.add_argument('--token', type=str, required=True)
        parser.add_argument('--url_domain', type=str, required=True)
        parser.add_argument('--out_pickle', type=str, required=True)
        args = parser.parse_args()
        
        with open(args.token, 'r') as f:
            token = f.read().strip()

        print(f"Fetching data for execution_id: {args.execution_id}")
        data = fetch_data_from_api(args.schema_id, args.execution_id, token, args.url_domain)
        print(f"API Response: {json.dumps(data, indent=2)}")
        
        # Extract content
        content = data.get('content', [])
        if not content:
            raise ValueError(f"No content found for execution_id: {args.execution_id}")
            
        inference_input_str = content[0].get('infernce_input', '[]')
        print(f"Raw inference_input string: {repr(inference_input_str)}")
        print(f"String length: {len(inference_input_str)}")
        
        # Robust parsing - try multiple methods
        inference_input = None
        try:
            # Method 1: Try ast.literal_eval first
            inference_input = ast.literal_eval(inference_input_str)
            print(" Parsed with ast.literal_eval")
        except (SyntaxError, ValueError) as e:
            print(f" ast.literal_eval failed: {e}")
            try:
                # Method 2: Try JSON parsing
                inference_input = json.loads(inference_input_str)
                print(" Parsed with json.loads")
            except json.JSONDecodeError as json_err:
                print(f" json.loads also failed: {json_err}")
                # Method 3: Try to fix common issues and retry
                try:
                    # Remove trailing commas or fix bracket issues
                    cleaned_str = inference_input_str.strip()
                    if cleaned_str.endswith(','):
                        cleaned_str = cleaned_str[:-1]
                    # Ensure proper bracket closure
                    open_brackets = cleaned_str.count('[')
                    close_brackets = cleaned_str.count(']')
                    if open_brackets > close_brackets:
                        cleaned_str += ']' * (open_brackets - close_brackets)
                    
                    print(f"Trying cleaned string: {repr(cleaned_str)}")
                    inference_input = json.loads(cleaned_str)
                    print(" Parsed with json.loads after cleaning")
                except Exception as final_err:
                    print(f" All parsing methods failed. Final error: {final_err}")
                    print(f"Original string: {repr(inference_input_str)}")
                    raise ValueError(f"Failed to parse inference_input: {final_err}")
        
        if inference_input is None:
            raise ValueError("Failed to parse inference_input - result is None")
            
        print(f" Successfully parsed inference input")
        print(f"   Type: {type(inference_input)}")
        print(f"   Length: {len(inference_input) if hasattr(inference_input, '__len__') else 'N/A'}")
        if hasattr(inference_input, '__len__') and len(inference_input) > 0:
            print(f"   First element type: {type(inference_input[0])}")
            if hasattr(inference_input[0], '__len__') and len(inference_input[0]) > 0:
                print(f"   Sequence length: {len(inference_input[0])}")
                if hasattr(inference_input[0][0], '__len__'):
                    print(f"   Features per timestep: {len(inference_input[0][0])}")
        
        os.makedirs(os.path.dirname(args.out_pickle), exist_ok=True)
        with open(args.out_pickle, "wb") as f:
            pickle.dump(inference_input, f)

        print(f"Saved pickled data to {args.out_pickle}")
    args:
      - --schema_id
      - {inputValue: schema_id}
      - --execution_id
      - {inputValue: execution_id}
      - --token
      - {inputPath: token}
      - --url_domain
      - {inputValue: url_domain}
      - --out_pickle
      - {outputPath: out_pickle}
