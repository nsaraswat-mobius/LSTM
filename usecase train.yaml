name: Train LSTM Model
description: Trains the LSTM model with support for Reddit adoption chasm classification and standard LSTM training.
inputs:
  - {name: model, type: Model}
  - {name: train_loader, type: Dataset}
  - {name: config, type: String}
outputs:
  - {name: trained_model, type: Model}
  - {name: epoch_loss, type: String}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v30
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import sys
        import torch
        import torch.nn as nn
        import numpy as np
        from sklearn.metrics import accuracy_score, classification_report, mean_squared_error
        
        from nesy_factory.RNNs import LSTM

        def calculate_adoption_metrics(predictions, targets, stage_labels=None, config=None):
            metrics = {}
            
            # Regression metrics (adoption score)
            mse = mean_squared_error(targets, predictions)
            rmse = np.sqrt(mse)
            mae = np.mean(np.abs(targets - predictions))
            
            metrics['mse'] = float(mse)
            metrics['rmse'] = float(rmse) 
            metrics['mae'] = float(mae)
            
            # Classification metrics (if stage labels available)
            if stage_labels is not None and config.get('task_type') == 'adoption_stage_classification':
                # Convert adoption scores to stage predictions using thresholds
                predicted_stages = []
                for score in predictions:
                    if score > 0.8:
                        predicted_stages.append('Innovators')
                    elif score > 0.6:
                        predicted_stages.append('Early_Adopters')
                    elif score > 0.4:
                        predicted_stages.append('Early_Majority')
                    elif score > 0.2:
                        predicted_stages.append('Late_Majority')
                    else:
                        predicted_stages.append('Laggards')
                
                # Calculate classification accuracy if we have ground truth stages
                try:
                    if hasattr(stage_labels, '__iter__') and len(stage_labels) == len(predicted_stages):
                        accuracy = accuracy_score(stage_labels, predicted_stages)
                        metrics['stage_accuracy'] = float(accuracy)
                        print(f"Stage Classification Accuracy: {accuracy:.4f}")
                except:
                    pass
            
            return metrics

        def train_adoption_lstm(model_obj, train_loader_obj, config):
            epochs = config.get('epochs', 100)
            learning_rate = config.get('learning_rate', 0.001)
            
            # Setup optimizer and loss function
            optimizer = torch.optim.Adam(model_obj.parameters(), lr=learning_rate, weight_decay=1e-5)
            
            if config.get('task_type') == 'adoption_stage_classification':
                # For adoption classification, use MSE for regression target (adoption_score)
                criterion = nn.MSELoss()
            else:
                # Default loss function from config or MSE
                loss_fn_name = config.get('loss_function', 'MSELoss')
                if loss_fn_name == 'BCEWithLogitsLoss':
                    criterion = nn.BCEWithLogitsLoss()
                elif loss_fn_name == 'CrossEntropyLoss':
                    criterion = nn.CrossEntropyLoss()
                else:
                    criterion = nn.MSELoss()
            
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='min', patience=10, factor=0.5, verbose=True
            )
            
            epoch_loss_data = []
            best_loss = float('inf')
            patience_counter = 0
            patience = 15
            
            print(f"Starting Enhanced LSTM Training for {config.get('use_case', 'Unknown')} use case")
            print(f"Task Type: {config.get('task_type', 'regression')}")
            print(f"Epochs: {epochs}, Learning Rate: {learning_rate}")
            print(f"Loss Function: {criterion.__class__.__name__}")
            
            for epoch in range(epochs):
                model_obj.train()
                total_train_loss = 0
                batch_count = 0
                all_predictions = []
                all_targets = []
                all_stage_labels = []
                
                for inputs, labels in train_loader_obj:
                    optimizer.zero_grad()
                    
                    # Forward pass
                    if hasattr(model_obj, 'forward'):
                        outputs = model_obj(inputs)
                    else:
                        # Use the model's train_step if available
                        loss = model_obj.train_step((inputs, labels))
                        total_train_loss += loss
                        batch_count += 1
                        continue
                    
                    # Calculate loss
                    if config.get('task_type') == 'adoption_stage_classification':
                        # For adoption classification, predict adoption scores
                        outputs = outputs.squeeze(-1) if outputs.dim() > 1 else outputs
                        labels = labels.squeeze(-1) if labels.dim() > 1 else labels
                    
                    loss = criterion(outputs, labels)
                    
                    # Backward pass
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model_obj.parameters(), max_norm=1.0)
                    optimizer.step()
                    
                    total_train_loss += loss.item()
                    batch_count += 1
                    
                    # Collect predictions for metrics
                    if config.get('use_case') == 'Reddit_Adoption_Chasm':
                        all_predictions.extend(outputs.detach().cpu().numpy())
                        all_targets.extend(labels.detach().cpu().numpy())
                
                avg_epoch_loss = total_train_loss / batch_count
                
                # Calculate additional metrics for adoption chasm
                metrics = {'loss': float(avg_epoch_loss)}
                if config.get('use_case') == 'Reddit_Adoption_Chasm' and all_predictions:
                    adoption_metrics = calculate_adoption_metrics(
                        np.array(all_predictions), 
                        np.array(all_targets), 
                        config=config
                    )
                    metrics.update(adoption_metrics)
                
                epoch_loss_data.append({
                    'epoch': epoch + 1,
                    **metrics
                })
                
                # Learning rate scheduling
                scheduler.step(avg_epoch_loss)
                
                # Early stopping
                if avg_epoch_loss < best_loss:
                    best_loss = avg_epoch_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                # Print progress
                if (epoch + 1) % 10 == 0 or epoch == 0:
                    print(f"Epoch [{epoch+1}/{epochs}] - Loss: {avg_epoch_loss:.6f}", end="")
                    if 'rmse' in metrics:
                        print(f" - RMSE: {metrics['rmse']:.4f}", end="")
                    if 'stage_accuracy' in metrics:
                        print(f" - Stage Acc: {metrics['stage_accuracy']:.4f}", end="")
                    print()
                
                # Early stopping check
                if patience_counter >= patience:
                    print(f"Early stopping at epoch {epoch+1}")
                    break
            
            return epoch_loss_data

        def train_standard_lstm(model_obj, train_loader_obj, config):
            epochs = config.get('epochs', 10)
            epoch_loss_data = []
            
            print("Starting Standard LSTM Model Training")
            
            for epoch in range(epochs):
                model_obj.train()
                total_train_loss = 0
                batch_count = 0
                
                for inputs, labels in train_loader_obj:
                    loss = model_obj.train_step((inputs, labels))
                    total_train_loss += loss
                    batch_count += 1
                
                avg_epoch_loss = total_train_loss / batch_count
                epoch_loss_data.append({
                    'epoch': epoch + 1,
                    'loss': float(avg_epoch_loss)
                })
                
                print(f"Epoch [{epoch+1}/{epochs}] - Average Loss: {avg_epoch_loss:.6f}")
            
            return epoch_loss_data

        parser = argparse.ArgumentParser()
        parser.add_argument('--model', type=str, required=True)
        parser.add_argument('--train_loader', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--epoch_loss', type=str, required=True)
        args = parser.parse_args()

        config = json.loads(args.config)

        with open(args.model, 'rb') as f:
            model_obj = pickle.load(f)

        with open(args.train_loader, 'rb') as f:
            train_loader_obj = pickle.load(f)

        # Choose training method based on use case
        if config.get('use_case') == 'Reddit_Adoption_Chasm' or config.get('task_type') == 'adoption_stage_classification':
            epoch_loss_data = train_adoption_lstm(model_obj, train_loader_obj, config)
            print("Reddit Adoption Chasm LSTM Training Completed")
        else:
            epoch_loss_data = train_standard_lstm(model_obj, train_loader_obj, config)
            print("Standard LSTM Model Training Completed")

        # Save trained model
        output_dir_trained_model = os.path.dirname(args.trained_model)
        if output_dir_trained_model and not os.path.exists(output_dir_trained_model):
            os.makedirs(output_dir_trained_model, exist_ok=True)
        with open(args.trained_model, 'wb') as f:
            pickle.dump(model_obj, f)

        # Save epoch loss data with enhanced metrics
        output_dir_epoch_loss = os.path.dirname(args.epoch_loss)
        if output_dir_epoch_loss and not os.path.exists(output_dir_epoch_loss):
            os.makedirs(output_dir_epoch_loss, exist_ok=True)
        
        # Add training summary
        training_summary = {
            'use_case': config.get('use_case', 'Unknown'),
            'task_type': config.get('task_type', 'regression'),
            'total_epochs': len(epoch_loss_data),
            'final_loss': epoch_loss_data[-1]['loss'] if epoch_loss_data else 0,
            'model_config': {
                'hidden_dim': config.get('hidden_dim', 'Unknown'),
                'num_layers': config.get('num_layers', 'Unknown'),
                'dropout': config.get('dropout', 'Unknown'),
                'learning_rate': config.get('learning_rate', 'Unknown')
            },
            'epoch_data': epoch_loss_data
        }
        
        with open(args.epoch_loss, 'w') as f:
            json.dump(training_summary, f, indent=2)

        print(f"Saved trained LSTM model to {args.trained_model}")
        print(f"Saved enhanced training metrics to {args.epoch_loss}")
        
        # Print final summary
        if epoch_loss_data:
            final_metrics = epoch_loss_data[-1]
            print(f"\nFinal Training Results:")
            print(f"  Final Loss: {final_metrics['loss']:.6f}")
            if 'rmse' in final_metrics:
                print(f"  Final RMSE: {final_metrics['rmse']:.4f}")
            if 'stage_accuracy' in final_metrics:
                print(f"  Final Stage Accuracy: {final_metrics['stage_accuracy']:.4f}")
    args:
      - --model
      - {inputPath: model}
      - --train_loader
      - {inputPath: train_loader}
      - --config
      - {inputValue: config}
      - --trained_model
      - {outputPath: trained_model}
      - --epoch_loss
      - {outputPath: epoch_loss}
