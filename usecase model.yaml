name: Initialize LSTM Model
description: Initializes the LSTM model with support for Reddit adoption chasm classification and standard LSTM configurations.
inputs:
  - {name: config, type: String}
outputs:
  - {name: model, type: Model}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v30
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import sys
        import torch
        import torch.nn as nn
        
        from nesy_factory.RNNs import LSTM

        def get_adoption_model_config(config):
            
            # Enhanced configuration for adoption chasm
            model_config = {
                'input_dim': len(config['feature_columns']),
                'hidden_dim': config.get('hidden_dim', 128),  # Larger for complex adoption patterns
                'output_dim': 1,  # Single adoption score output
                'num_layers': config.get('num_layers', 2),  # Deeper for sequential patterns
                'dropout': config.get('dropout', 0.3),  # Regularization for adoption data
                'optimizer': 'adam',
                'learning_rate': config.get('learning_rate', 0.001),
                'epochs': config.get('epochs', 100),  # More epochs for adoption learning
                'loss_function': config.get('loss_function', 'MSELoss'),  # Regression for adoption score
                'bidirectional': config.get('bidirectional', True),  # Better context understanding
                'batch_first': True,
                'use_case': config.get('use_case', 'Reddit_Adoption_Chasm'),
                'task_type': config.get('task_type', 'adoption_stage_classification')
            }
            
            # Adjust output_dim based on task type
            if config.get('task_type') == 'adoption_stage_classification':
                # Single continuous output for adoption score (0-1)
                model_config['output_dim'] = 1
                model_config['activation'] = 'sigmoid'  # Ensure 0-1 range
            elif config.get('task_type') == 'multi_stage_classification':
                # 5 classes for direct stage prediction
                model_config['output_dim'] = 5
                model_config['activation'] = 'softmax'
                model_config['loss_function'] = 'CrossEntropyLoss'
            
            return model_config

        def get_standard_model_config(config):
            
            model_config = {
                'input_dim': len(config['feature_columns']),
                'hidden_dim': config.get('hidden_dim', 64),
                'output_dim': config.get('output_dim', 1),
                'num_layers': config.get('num_layers', 1),
                'dropout': config.get('dropout', 0.2),
                'optimizer': config.get('optimizer', 'adam'),
                'learning_rate': config.get('learning_rate', 0.01),
                'epochs': config.get('epochs', 10),
                'loss_function': config.get('loss_function', 'MSELoss'),
                'bidirectional': config.get('bidirectional', False),
                'batch_first': True,
                'use_case': config.get('use_case', 'Unknown'),
                'task_type': config.get('task_type', 'regression')
            }
            
            return model_config

        class EnhancedLSTM(nn.Module):
            
            def __init__(self, config):
                super(EnhancedLSTM, self).__init__()
                self.config = config
                self.use_case = config.get('use_case', 'Unknown')
                self.task_type = config.get('task_type', 'regression')
                
                # LSTM layer
                self.lstm = nn.LSTM(
                    input_size=config['input_dim'],
                    hidden_size=config['hidden_dim'],
                    num_layers=config['num_layers'],
                    dropout=config['dropout'] if config['num_layers'] > 1 else 0,
                    bidirectional=config.get('bidirectional', False),
                    batch_first=config.get('batch_first', True)
                )
                
                # Calculate LSTM output size
                lstm_output_size = config['hidden_dim']
                if config.get('bidirectional', False):
                    lstm_output_size *= 2
                
                # Output layers for adoption chasm
                if self.use_case == 'Reddit_Adoption_Chasm':
                    self.dropout = nn.Dropout(config.get('dropout', 0.3))
                    self.fc1 = nn.Linear(lstm_output_size, lstm_output_size // 2)
                    self.fc2 = nn.Linear(lstm_output_size // 2, config['output_dim'])
                    self.activation = nn.Sigmoid() if config.get('activation') == 'sigmoid' else nn.Identity()
                    self.relu = nn.ReLU()
                else:
                    # Standard output layer
                    self.fc = nn.Linear(lstm_output_size, config['output_dim'])
                    self.activation = nn.Identity()
                
                # Loss function and optimizer setup
                self.setup_training_components()
            
            def setup_training_components(self):
                # Loss function
                loss_fn_name = self.config.get('loss_function', 'MSELoss')
                if loss_fn_name == 'MSELoss':
                    self.criterion = nn.MSELoss()
                elif loss_fn_name == 'BCEWithLogitsLoss':
                    self.criterion = nn.BCEWithLogitsLoss()
                elif loss_fn_name == 'CrossEntropyLoss':
                    self.criterion = nn.CrossEntropyLoss()
                else:
                    self.criterion = nn.MSELoss()
                
                # Optimizer
                if self.config.get('optimizer', 'adam').lower() == 'adam':
                    self.optimizer = torch.optim.Adam(
                        self.parameters(), 
                        lr=self.config.get('learning_rate', 0.001),
                        weight_decay=1e-5
                    )
                else:
                    self.optimizer = torch.optim.SGD(
                        self.parameters(), 
                        lr=self.config.get('learning_rate', 0.01)
                    )
            
            def forward(self, x):
                # LSTM forward pass
                lstm_out, (hidden, cell) = self.lstm(x)
                
                # Use the last time step output
                if self.config.get('batch_first', True):
                    output = lstm_out[:, -1, :]  # (batch_size, hidden_size)
                else:
                    output = lstm_out[-1, :, :]  # (batch_size, hidden_size)
                
                # Apply output layers based on use case
                if self.use_case == 'Reddit_Adoption_Chasm':
                    output = self.dropout(output)
                    output = self.relu(self.fc1(output))
                    output = self.fc2(output)
                    output = self.activation(output)
                else:
                    output = self.fc(output)
                
                return output
            
            def train_step(self, batch):
                inputs, targets = batch
                
                self.train()
                self.optimizer.zero_grad()
                
                # Forward pass
                outputs = self.forward(inputs)
                
                # Ensure compatible shapes for loss calculation
                if self.task_type == 'adoption_stage_classification':
                    outputs = outputs.squeeze(-1) if outputs.dim() > 1 else outputs
                    targets = targets.squeeze(-1) if targets.dim() > 1 else targets
                
                # Calculate loss
                loss = self.criterion(outputs, targets)
                
                # Backward pass
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
                self.optimizer.step()
                
                return loss.item()

        def create_model(config):
            
            if config.get('use_case') == 'Reddit_Adoption_Chasm' or config.get('task_type') == 'adoption_stage_classification':
                # Use enhanced LSTM for adoption chasm
                model_config = get_adoption_model_config(config)
                model_obj = EnhancedLSTM(model_config)
                print(f"Created Enhanced LSTM for Reddit Adoption Chasm Classification")
                print(f"  Input Dimension: {model_config['input_dim']}")
                print(f"  Hidden Dimension: {model_config['hidden_dim']}")
                print(f"  Output Dimension: {model_config['output_dim']}")
                print(f"  Number of Layers: {model_config['num_layers']}")
                print(f"  Bidirectional: {model_config.get('bidirectional', False)}")
                print(f"  Task Type: {model_config.get('task_type', 'adoption_stage_classification')}")
                
            else:
                # Use standard LSTM for other use cases
                model_config = get_standard_model_config(config)
                model_obj = LSTM(model_config)
                print(f"Created Standard LSTM for {config.get('use_case', 'Unknown')} use case")
                print(f"  Input Dimension: {model_config['input_dim']}")
                print(f"  Hidden Dimension: {model_config['hidden_dim']}")
                print(f"  Output Dimension: {model_config['output_dim']}")
                print(f"  Number of Layers: {model_config['num_layers']}")
            
            return model_obj

        parser = argparse.ArgumentParser()
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--model', type=str, required=True)
        args = parser.parse_args()

        config = json.loads(args.config)
        
        print(f"Initializing LSTM Model for use case: {config.get('use_case', 'Unknown')}")
        print(f"Task type: {config.get('task_type', 'regression')}")
        print(f"Number of features: {len(config['feature_columns'])}")

        # Create the appropriate model
        model_obj = create_model(config)

        # Save the model
        os.makedirs(os.path.dirname(args.model), exist_ok=True)
        with open(args.model, "wb") as f:
            pickle.dump(model_obj, f)

        print(f"Successfully saved LSTM model to {args.model}")
        
        # Print model summary
        if hasattr(model_obj, 'config'):
            model_config = model_obj.config
        else:
            model_config = config
            
        print(f"\\nModel Configuration Summary:")
        for key, value in model_config.items():
            if key != 'feature_columns':  # Skip long feature list
                print(f"  {key}: {value}")
    args:
      - --config
      - {inputValue: config}
      - --model
      - {outputPath: model}
